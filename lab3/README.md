[![Open in Visual Studio Code](https://classroom.github.com/assets/open-in-vscode-718a45dd9cf7e7f842a935f5ebbe5719a5e09af4491e668f4dbf3b35d5cca122.svg)](https://classroom.github.com/online_ide?assignment_repo_id=11137739&assignment_repo_type=AssignmentRepo)
# Лабораторная работа по курсу "Искусственный интеллект"
# Создание своего нейросетевого фреймворка

### Студенты: 

| ФИО       | Роль в проекте                     | Оценка       |
|-----------|------------------------------------|--------------|
| Семин А.В | Программировал |       |
| Андреев А.О| Программировал оптимизаторы |      |
| Попов И.П   | Писал отчёт |          |

> *Комментарии проверяющего*

### Оптимизаторы
#### Стохастический градиентный спуск
Стохастический градиентный спуск — итерационный метод для оптимизации целевой функции с подходящими свойствами гладкости (например, дифференцируемость или субдифференцируемость). Его можно расценивать как стохастическую аппроксимацию оптимизации методом градиентного спуска, поскольку он заменяет реальный градиент, вычисленный из полного набора данных, оценкой, вычисленной из случайно выбранного подмножества данных. Это сокращает задействованные вычислительные ресурсы и помогает достичь более высокой скорости итераций в обмен на более низкую скорость сходимости. Особенно большой эффект достигается в приложениях, связанных с обработкой больших данных.
![image](https://github.com/MAILabs-Edu-2023/ai-lab3-ai-lab-3-andreev-popov-semin/assets/91149939/b3e38d2a-b932-4734-b084-1a6d5d274898)
#### Momentum sgd
Более поздние разработки включают метод импульса, который появился в статье Румельхарта, Хинтона и Уильямса об обучении с обратным распространением ошибки. Стохастический градиентный спуск с импульсом запоминает изменение Δw на каждой итерации и определяет следующее изменение в виде линейной комбинации градиента и предыдущего изменения:
![image](https://github.com/MAILabs-Edu-2023/ai-lab3-ai-lab-3-andreev-popov-semin/assets/91149939/6cc8f3ef-56ec-4cbe-a757-7f00bd189a9c)
#### Gradient clipping
Отсечение градиента - это метод, при котором производная ошибки изменяется или обрезается до порогового значения во время обратного распространения по сети и используется отсеченные градиенты для обновления весов.

При изменении масштаба производной ошибки обновления весов также будут изменены, что значительно снизит вероятность переполнения или недостаточного потока.
![image](https://github.com/MAILabs-Edu-2023/ai-lab3-ai-lab-3-andreev-popov-semin/assets/91149939/725b66e5-d79c-47bb-ba21-04611f53b622)

### Передаточные функции
#### ReLU
В искусственных нейронных сетях функция активации нейрона определяет выходной сигнал, который определяется входным сигналом или набором входных сигналов. Стандартная компьютерная микросхема может рассматриваться как цифровая сеть функций активации, которые могут принимать значения «ON» (1) или «OFF» (0) в зависимости от входа. Это похоже на поведение линейного перцептрона в нейронных сетях. Однако только нелинейные функции активации позволяют таким сетям решать нетривиальные задачи с использованием малого числа узлов. В искусственных нейронных сетях эта функция также называется передаточной функцией.
#### Tanh
Активация Tanh - это функция активации, используемая для нейронных сетей:

![image](https://github.com/MAILabs-Edu-2023/ai-lab3-ai-lab-3-andreev-popov-semin/assets/91149939/8f7f2ec8-8106-4699-814a-b88db8f21d27)
#### Sigmoid
Эта функция принимает любое значение в качестве входных данных и выводит значения в диапазоне от 0 до 1. Чем больше входное значение (более положительное), тем ближе выходное значение будет к 1, тогда как чем меньше входное значение (более отрицательное), тем ближе выходное значение будет к 0

![image](https://github.com/MAILabs-Edu-2023/ai-lab3-ai-lab-3-andreev-popov-semin/assets/91149939/2abfaef7-ba3f-471b-93b8-1a331acb6dc3)
#### Softmax
Функция softmax преобразует вектор из K действительных чисел в распределение вероятностей из K возможных исходов. Это обобщение логистической функции на множество измерений и используется в мультиномиальной логистической регрессии. Функция softmax часто используется в качестве последней функции активации нейронной сети для приведения выходных данных сети к распределению вероятностей по прогнозируемым классам выходных данных.

![image](https://github.com/MAILabs-Edu-2023/ai-lab3-ai-lab-3-andreev-popov-semin/assets/91149939/0d2f5c8a-17ab-4e27-ac08-599a17e6caf5)

### Функции ошибок
#### Binary Cross-Entropy
Двоичная перекрестная энтропия - это модельный показатель, который отслеживает неправильную маркировку класса данных моделью, наказывая модель, если при классификации меток возникают отклонения в вероятности. Низкие значения потерь в логарифмах соответствуют высоким значениям точности.

![image](https://github.com/MAILabs-Edu-2023/ai-lab3-ai-lab-3-andreev-popov-semin/assets/91149939/292d56dc-d3a2-4d28-ba0e-3d0e2bdbd042)

#### Cross-Entropy Loss
В теории информации перекрёстная энтропия между двумя распределениями вероятностей измеряет среднее число бит, необходимых для опознания события из набора возможностей, если используемая схема кодирования базируется на заданном распределении вероятностей q, вместо «истинного» распределения p.

Перекрестная энтропия для двух распределений p и q над одним и тем же вероятностным пространством определяется следующим образом:

![image](https://github.com/MAILabs-Edu-2023/ai-lab3-ai-lab-3-andreev-popov-semin/assets/91149939/e2fa4b58-d31f-496a-92a9-758e5ae74cc7)

#### Mean Squared Error
В статистике среднеквадратичная ошибка измеряет среднее значение квадратов ошибок — то есть разницу в среднем квадрате между оценочными значениями и фактическим значением.

![image](https://github.com/MAILabs-Edu-2023/ai-lab3-ai-lab-3-andreev-popov-semin/assets/91149939/1ad324d6-84ae-4e33-98dd-e444ed048868)

### Вывод
В ходе этой лабораторной работы мы научились создавать многослойные нейросети перечислением слоев, используя удобный набор функций для работы с данными и датасетами(map, minibatching, перемешивание и др.).

Мы реализовали три алгоритма оптимизации, такие как: SGD, Momentum SGD, Gradient Clipping.

Также изучили и написали передаточные функции(ReLU, Tanh, Sigmoid, Softmax) и функции ошибок(Binary Cross-Entropy, Cross-Entropy Loss, Mean Squared Error).
